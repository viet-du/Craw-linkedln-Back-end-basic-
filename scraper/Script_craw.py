# Import libraries and packages for the project 
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium import webdriver
from bs4 import BeautifulSoup
from time import sleep
import time
import json
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random
import os
#Th√™m kafka
from kafka import KafkaProducer
#Th√™m tho√°t craw ƒëa thread
import signal
import sys
#Th√™m threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from selenium.common.exceptions import TimeoutException
import hashlib
import json
from datetime import datetime, timedelta
file_lock = threading.Lock()
output_path = r"D:\Hoc_tap\linkedlin\Data\output.json"
# ===== META DATA CONFIG =====
META_PATH = r"D:\Hoc_tap\linkedlin\Data\crawl_meta.json"  # file l∆∞u th√¥ng tin crawl
MAX_AGE_DAYS = 30  # s·ªë ng√†y t·ªëi ƒëa tr∆∞·ªõc khi crawl l·∫°i
stop_flag = False

def signal_handler(sig, frame):
    global stop_flag
    print("\nüõë ƒêang d·ª´ng an to√†n... Vui l√≤ng ch·ªù ho√†n t·∫•t profile hi·ªán t·∫°i...")
    stop_flag = True

signal.signal(signal.SIGINT, signal_handler)

# TƒÉng th·ªùi gian timeout
WAIT_TIMEOUT = 30
def calculate_checksum(profile_data):
    """T·∫°o hash t·ª´ c√°c tr∆∞·ªùng quan tr·ªçng ƒë·ªÉ ph√°t hi·ªán thay ƒë·ªïi."""
    important = {
        'name': profile_data.get('name'),
        'job_title': profile_data.get('job_title'),
        'location': profile_data.get('location'),
        'experience': profile_data.get('experience'),
        'education': profile_data.get('education')
    }
    return hashlib.sha256(json.dumps(important, sort_keys=True).encode()).hexdigest()
# Task 1: Login to Linkedin
print("=== B·∫Øt ƒë·∫ßu ƒëƒÉng nh·∫≠p LinkedIn ===")
print("L∆∞u √Ω: B·∫°n c√≥ 60 gi√¢y ƒë·ªÉ gi·∫£i captcha n·∫øu c√≥")

# Task 1.1: Open Chrome and Access Linkedin login site
driver = webdriver.Chrome()
sleep(2)
url = 'https://www.linkedin.com/checkpoint/lg/sign-in-another-account?trk=guest_homepage-basic_nav-header-signin'
driver.get(url)
print('- Finish initializing a driver')
sleep(2)

# Task 1.2: Import username and password
credential_path = r"D:\Hoc_tap\linkedlin\scraper\login.txt"
try:
    with open(credential_path, "r", encoding="utf-8") as credential:
        lines = credential.read().splitlines()
        username = lines[0] if len(lines) > 0 else ""
        password = lines[1] if len(lines) > 1 else ""
    print('- Finish importing the login credentials')
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {credential_path}")
    username = input("Nh·∫≠p username/email: ")
    password = input("Nh·∫≠p password: ")

sleep(2)

# Task 1.2: Key in login credentials
try:
    email_field = driver.find_element(By.ID, "username")
    email_field.send_keys(username)
    print('- Finish keying in email')
    sleep(2)

    password_field = driver.find_element(By.NAME, "session_password")
    password_field.send_keys(password)
    print('- Finish keying in password')
    sleep(2)
    
    # Task 1.2: Click the Login button
    signin_field = driver.find_element(By.XPATH, '//*[@id="organic-div"]/form/div[3]/button')
    signin_field.click()
    print('ƒê√£ click ƒëƒÉng nh·∫≠p. Vui l√≤ng gi·∫£i captcha trong 60 gi√¢y n·∫øu c√≥...')
    
    # TƒÇNG TH·ªúI GIAN CH·ªú ƒêƒÇNG NH·∫¨P L√äN 60 GI√ÇY ƒê·ªÇ GI·∫¢I CAPTCHA
    sleep(60)
    
    print('- Finish Task 1: Login to Linkedin')
    #l∆∞u cookie cho c√°c threading sau
    cookies = driver.get_cookies()
    # ===== KAFKA CONFIG =====
    KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
    KAFKA_TOPIC = "linkedin-profiles"

    producer = KafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode("utf-8"),
        retries=5,
        acks='all'
    )

    print("‚úÖ Kafka Producer ƒë√£ kh·ªüi t·∫°o")

except Exception as e:
    print(f"L·ªói ƒëƒÉng nh·∫≠p: {e}")

# Task 2: Search for the profile we want to crawl + button people
print("\n=== B·∫Øt ƒë·∫ßu t√¨m ki·∫øm profiles ===")
profiles_path = r"D:\Hoc_tap\linkedlin\scraper\profiles.txt"
try:
    with open(profiles_path, "r", encoding="utf-8") as f:
        profiles = [line.strip() for line in f if line.strip()]
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {profiles_path}")
    profiles = ["data scientist"]

for profile in profiles:
    print(f"ƒêang t√¨m ki·∫øm: {profile}")
    
    try:
        search_field = WebDriverWait(driver, WAIT_TIMEOUT).until(
            EC.element_to_be_clickable(
                (By.CSS_SELECTOR, "input[data-testid='typeahead-input']")
            )
        )

        search_field.click()
        time.sleep(1)

        search_field.clear()
        search_field.send_keys(profile)
        search_field.send_keys(Keys.ENTER)

        print(f"  ‚úÖ ƒê√£ t√¨m ki·∫øm: {profile}")
        time.sleep(5)

    except Exception as e:
        print(f"  ‚ùå Kh√¥ng t√¨m ƒë∆∞·ª£c √¥ search: {e}")
        
        # D·ª´ng l·∫°i ƒë·ªÉ b·∫°n c√≥ th·ªÉ t·ª± nh·∫•n n√∫t 'Ng∆∞·ªùi' n·∫øu mu·ªën
        print("  N·∫øu b·∫°n mu·ªën l·ªçc 'Ng∆∞·ªùi', vui l√≤ng nh·∫•n tab 'Ng∆∞·ªùi' tr√™n tr√¨nh duy·ªát ngay b√¢y gi·ªù.")
        print("  Khi ƒë√£ s·∫µn s√†ng, quay v·ªÅ terminal v√† nh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c (ho·∫∑c nh·∫•n Enter ngay l·∫≠p t·ª©c ƒë·ªÉ b·ªè qua).")
        try:
            input("  Nh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c...")
        except Exception:
            # Trong m·ªôt s·ªë m√¥i tr∆∞·ªùng kh√¥ng h·ªó tr·ª£ input(), ti·∫øp t·ª•c ngay
            pass

        # Scroll ƒë·ªÉ load profiles
        for i in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        # L·∫•y v√† hi·ªÉn th·ªã m·ªôt s·ªë profiles ƒë·ªÉ ki·ªÉm tra
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")

        profile_cards = soup.find_all("div", {"class": ["search-result", "reusable-search__result-container", "entity-result"]})
        print(f"  T√¨m th·∫•y {len(profile_cards)} profile cards")

        if len(profile_cards) == 0:
            print("  ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y profile cards n√†o!")
            print("  ƒêang th·ª≠ t√¨m profiles b·∫±ng c√°ch kh√°c...")
            profile_links = soup.find_all("a", href=lambda x: x and "/in/" in x)
            print(f"  T√¨m th·∫•y {len(profile_links)} link profiles tr·ª±c ti·∫øp")

        time.sleep(3)
        
    except Exception as e:
        print(f"  L·ªói khi t√¨m ki·∫øm {profile}: {e}")
        print("  Ti·∫øp t·ª•c v·ªõi profile ti·∫øp theo...")
        continue

print("\n=== Ho√†n th√†nh Task 2: T√¨m ki·∫øm profiles ===")

# Task 3: Scrape the URLs of the profiles
print("\n=== B·∫Øt ƒë·∫ßu thu th·∫≠p URLs ===")

def GetURL():
    page_source = BeautifulSoup(driver.page_source, "html.parser")
    urls = []
    
    # T√¨m t·∫•t c·∫£ c√°c th·∫ª c√≥ ch·ª©a link profile
    profile_links = page_source.find_all("a", href=lambda x: x and "/in/" in x)
    
    for link in profile_links:
        href = link.get('href', '')
        if '/in/' in href:
            clean_url = href.split('?')[0]
            if not clean_url.startswith('http'):
                clean_url = 'https://www.linkedin.com' + clean_url
            
            if clean_url not in urls and 'linkedin.com/in/' in clean_url:
                urls.append(clean_url)
    
    print(f"  T√¨m th·∫•y {len(urls)} URLs trong trang hi·ªán t·∫°i")
    return urls

try:
    input_page = int(input('\nB·∫°n mu·ªën crawl bao nhi√™u trang? '))
except:
    input_page = 2

URLs_all_page = []

for page in range(input_page):
    print(f"\nƒêang x·ª≠ l√Ω trang {page + 1}/{input_page}")
    URLs_one_page = GetURL()
    URLs_all_page.extend(URLs_one_page)
    
    print(f"  ƒê√£ thu th·∫≠p {len(URLs_one_page)} URLs t·ª´ trang n√†y")
    print(f"  T·ªïng URLs ƒë√£ thu th·∫≠p: {len(URLs_all_page)}")
    
    # Scroll ƒë·ªÉ load th√™m
    sleep(3)
    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
    sleep(3)
    
    # ƒê·ª¢I V√Ä T√åM N√öT NEXT ƒê·ªÇ CHUY·ªÇN TRANG
    if page < input_page - 1:
        print("  ƒêang t√¨m n√∫t Next ƒë·ªÉ chuy·ªÉn trang...")
        next_clicked = False
        
        # PH∆Ø∆†NG PH√ÅP 1: T√¨m b·∫±ng data-testid (THEO HTML)
        try:
            next_button = driver.find_element(By.CSS_SELECTOR, "button[data-testid='pagination-controls-next-button-visible']")
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)
            sleep(2)
            
            # Ki·ªÉm tra xem n√∫t c√≥ b·ªã disabled kh√¥ng
            if not next_button.get_attribute("disabled"):
                driver.execute_script("arguments[0].click();", next_button)
                next_clicked = True
                print("  ƒê√£ nh·∫•n n√∫t Next b·∫±ng data-testid")
            else:
                print("  N√∫t Next b·ªã disabled, kh√¥ng c√≥ trang ti·∫øp theo")
                break
        except Exception as e:
            print(f"  Kh√¥ng t√¨m th·∫•y n√∫t b·∫±ng data-testid: {e}")
        
        # PH∆Ø∆†NG PH√ÅP 2: T√¨m b·∫±ng text "Ti·∫øp theo" (fallback)
        if not next_clicked:
            try:
                next_button = driver.find_element(By.XPATH, "//button[contains(., 'Ti·∫øp theo')]")
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)
                sleep(2)
                
                if not next_button.get_attribute("disabled"):
                    driver.execute_script("arguments[0].click();", next_button)
                    next_clicked = True
                    print("  ƒê√£ nh·∫•n n√∫t Next b·∫±ng text 'Ti·∫øp theo'")
                else:
                    print("  N√∫t Next b·ªã disabled")
                    break
            except:
                pass
        
        # PH∆Ø∆†NG PH√ÅP 3: T√¨m b·∫±ng aria-label (fallback)
        if not next_clicked:
            try:
                next_button = driver.find_element(By.XPATH, "//button[@aria-label='Next']")
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_button)
                sleep(2)
                
                if not next_button.get_attribute("disabled"):
                    driver.execute_script("arguments[0].click();", next_button)
                    next_clicked = True
                    print("  ƒê√£ nh·∫•n n√∫t Next b·∫±ng aria-label")
                else:
                    print("  N√∫t Next b·ªã disabled")
                    break
            except:
                pass
        
        if not next_clicked:
            print("  KH√îNG t√¨m th·∫•y n√∫t Next. D·ª´ng thu th·∫≠p.")
            break
        
        # ƒê·ª£i trang m·ªõi load ho√†n to√†n
        print("  ƒêang ch·ªù trang m·ªõi load...")
        time.sleep(5)
        
        # Ch·ªù cho ƒë·∫øn khi c√≥ k·∫øt qu·∫£ m·ªõi xu·∫•t hi·ªán
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.search-results-container, div.reusable-search__entity-result-list"))
            )
            print("  Trang m·ªõi ƒë√£ load xong")
        except:
            print("  Ch·ªù trang m·ªõi load timeout, ti·∫øp t·ª•c...")

URLs_all_page = list(set(URLs_all_page))

# Ki·ªÉm tra file output hi·ªán c√≥ ƒë·ªÉ b·ªè qua nh·ªØng URL ƒë√£ crawl tr∆∞·ªõc ƒë√≥
# ===== ƒê·ªåC METADATA =====
meta = {}
if os.path.exists(META_PATH):
    try:
        with open(META_PATH, 'r', encoding='utf-8') as f:
            meta = json.load(f)
    except:
        meta = {}

# K·∫øt h·ª£p metadata v·ªõi output.json (n·∫øu c√≥)
existing_urls = set()
if os.path.exists(output_path):
    with open(output_path, 'r', encoding='utf-8') as ef:
        try:
            existing_data = json.load(ef)
            for item in existing_data:
                u = item.get('url') or item.get('linkedin_url')
                if u:
                    u_clean = u.split('?')[0]
                    existing_urls.add(u_clean)
                    # N·∫øu ch∆∞a c√≥ metadata, t·∫°o m·ªõi
                    if u_clean not in meta:
                        meta[u_clean] = {
                            'last_crawled': None,
                            'checksum': None
                        }
        except:
            pass

# L·ªçc URL m·ªõi (ch∆∞a t·ª´ng crawl) v√† URL c≈© c·∫ßn c·∫≠p nh·∫≠t
new_urls = []          # ch∆∞a c√≥ trong meta
update_urls = []       # ƒë√£ c√≥ nh∆∞ng qu√° h·∫°n ho·∫∑c c·∫ßn c·∫≠p nh·∫≠t
now = datetime.now()

for u in URLs_all_page:
    u_clean = u.split('?')[0]
    if u_clean not in meta:
        new_urls.append(u)
    else:
        last = meta[u_clean].get('last_crawled')
        if last:
            last_date = datetime.fromisoformat(last)
            if now - last_date > timedelta(days=MAX_AGE_DAYS):
                update_urls.append(u)   # ƒë√£ qu√° h·∫°n, c·∫ßn crawl l·∫°i
        else:
            update_urls.append(u)       # ch∆∞a c√≥ th·ªùi gian (d·ªØ li·ªáu c≈©)

print(f"‚úÖ URL m·ªõi: {len(new_urls)}")
print(f"üîÑ URL c·∫ßn c·∫≠p nh·∫≠t (c≈© h∆°n {MAX_AGE_DAYS} ng√†y): {len(update_urls)}")

# K·∫øt h·ª£p ƒë·ªÉ crawl
crawl_urls = new_urls + update_urls
print(f"üìå T·ªïng s·ªë URL s·∫Ω crawl: {len(crawl_urls)}")

print("\n5 URLs ƒë·∫ßu ti√™n:")
for i, url in enumerate(URLs_all_page[:5]):
    print(f"  {i+1}. {url}")


def crawl_profile(linkedin_URL, idx, total_profiles):
    global stop_flag
    
    if stop_flag:
        return None
    options = webdriver.ChromeOptions()

# Headless mode
    options.add_argument("--headless=new")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-gpu")

    # Gi·∫£ l·∫≠p k√≠ch th∆∞·ªõc window ·ªïn ƒë·ªãnh
    options.add_argument("--window-size=1280,900")
    #Gi·∫£m b·ªã detect automation.
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-blink-features=AutomationControlled")


    thread_driver = webdriver.Chrome(options=options)



    # G·∫Øn l·∫°i cookies ƒë·ªÉ kh·ªèi login l·∫°i
    thread_driver.get("https://www.linkedin.com")
    for cookie in cookies:
        thread_driver.add_cookie(cookie)

    try:
        print(f"\n[{idx}/{total_profiles}] ƒêang x·ª≠ l√Ω: {linkedin_URL}")

        thread_driver.set_page_load_timeout(60)
        thread_driver.get(linkedin_URL)

        WebDriverWait(thread_driver, WAIT_TIMEOUT).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        time.sleep(random.uniform(5, 8))  # tƒÉng delay

        # Scroll ch·∫≠m h∆°n
        for i in range(2):
            thread_driver.execute_script(
                "window.scrollTo(0, document.body.scrollHeight);"
            )
            time.sleep(random.uniform(1.5, 2.5))


        soup = BeautifulSoup(thread_driver.page_source, "html.parser")
        
        # C·∫§U TR√öC D·ªÆ LI·ªÜU
        profile_data = {
            "name": None,
            "location": None,
            "job_title": None,
            "education": [],
            "experience": [],
            "total_experience_count": 0,
            "url": linkedin_URL
        }
        checksum = calculate_checksum(profile_data)
        profile_data['_checksum'] = checksum
        # 1. L·∫§Y T√äN
        try:
            name_element = soup.find("h1", {"class": lambda x: x and any(cls in str(x) for cls in ["text-heading-xlarge", "t-24", "inline"])})
            if name_element:
                profile_data["name"] = name_element.get_text(strip=True)
        except Exception as e:
            print(f"    L·ªói khi l·∫•y t√™n: {e}")
        
        # 2. L·∫§Y CH·ª®C V·ª§ HI·ªÜN T·∫†I
        try:
            job_element = soup.find("div", {"class": lambda x: x and any(cls in str(x) for cls in ["text-body-medium", "break-words"])})
            if job_element:
                profile_data["job_title"] = job_element.get_text(strip=True)
        except Exception as e:
            print(f"    L·ªói khi l·∫•y ch·ª©c v·ª•: {e}")
        
        # 3. L·∫§Y ƒê·ªäA ƒêI·ªÇM (LOCATION)
        try:
            location_element = soup.find("span", {"class": lambda x: x and "text-body-small" in str(x) and "t-black--light" in str(x) and "break-words" in str(x)})
            if location_element:
                profile_data["location"] = location_element.get_text(strip=True)
        except Exception as e:
            print(f"    L·ªói khi l·∫•y ƒë·ªãa ƒëi·ªÉm: {e}")
        
        # 4. L·∫§Y KINH NGHI·ªÜM (EXPERIENCE) - C√ÅCH TI·∫æP C·∫¨N M·ªöI
        print("    ƒêang l·∫•y th√¥ng tin kinh nghi·ªám...")
        try:
            # T√¨m t·∫•t c·∫£ c√°c section v√† l·ªçc ra section kinh nghi·ªám
            all_sections = soup.find_all("section")
            
            for section in all_sections:
                # Ki·ªÉm tra ti√™u ƒë·ªÅ c·ªßa section
                section_title = None
                h2_tag = section.find("h2")
                if h2_tag:
                    section_title = h2_tag.get_text(strip=True).lower()
                
                # Ki·ªÉm tra n·∫øu l√† section kinh nghi·ªám
                if section_title and ("experience" in section_title or "kinh nghi·ªám" in section_title or "work" in section_title):
                    print(f"    T√¨m th·∫•y section kinh nghi·ªám: {section_title}")
                    
                    # T√¨m t·∫•t c·∫£ c√°c m·ª•c trong section n√†y
                    # Th·ª≠ nhi·ªÅu c·∫•u tr√∫c kh√°c nhau
                    exp_items = []
                    
                    # C√°ch 1: T√¨m c√°c li items
                    exp_items = section.find_all("li", {"class": lambda x: x and any(cls in str(x) for cls in ["artdeco-list__item", "pvs-list__item"])})
                    
                    # C√°ch 2: T√¨m c√°c div c√≥ c·∫•u tr√∫c c·ª• th·ªÉ
                    if not exp_items:
                        exp_items = section.find_all("div", {"class": lambda x: x and "display-flex flex-column" in str(x)})
                    
                    # C√°ch 3: T√¨m t·∫•t c·∫£ c√°c div v√† l·ªçc
                    if not exp_items:
                        all_divs = section.find_all("div")
                        for div in all_divs:
                            # Ki·ªÉm tra div c√≥ ch·ª©a th√¥ng tin c√¥ng vi·ªác kh√¥ng
                            has_position = div.find("div", {"class": lambda x: x and "hoverable-link-text" in str(x) and "t-bold" in str(x)})
                            has_company = div.find("span", {"class": lambda x: x and "t-14" in str(x) and "t-normal" in str(x)})
                            if has_position or has_company:
                                exp_items.append(div)
                    
                    print(f"    T√¨m th·∫•y {len(exp_items)} m·ª•c kinh nghi·ªám")
                    
                    for item in exp_items:
                        # B·ªè qua c√°c item qu√° nh·ªè ho·∫∑c kh√¥ng c√≥ th√¥ng tin
                        item_text = item.get_text(strip=True)
                        if len(item_text) < 10:
                            continue
                        
                        exp_data = {
                            "position": None,
                            "company": None,
                            "employment_type": None,
                            "duration": None
                        }
                        
                        # T√¨m ch·ª©c danh (position)
                        try:
                            # T√¨m trong th·∫ª span v·ªõi aria-hidden="true" trong div c√≥ class hoverable-link-text t-bold
                            pos_div = item.find("div", {"class": lambda x: x and "hoverable-link-text" in str(x) and "t-bold" in str(x)})
                            if pos_div:
                                pos_span = pos_div.find("span", {"aria-hidden": "true"})
                                if pos_span:
                                    pos_text = pos_span.get_text(strip=True)
                                    if pos_text:
                                        exp_data["position"] = pos_text
                        except:
                            pass
                        
                        # T√¨m c√¥ng ty v√† lo·∫°i h√¨nh l√†m vi·ªác
                        try:
                            # T√¨m span c√≥ class t-14 t-normal
                            company_span = item.find("span", {"class": lambda x: x and "t-14" in str(x) and "t-normal" in str(x)})
                            if company_span:
                                company_text = company_span.get_text(strip=True)
                                if company_text and "¬∑" in company_text:
                                    parts = company_text.split("¬∑")
                                    if len(parts) >= 2:
                                        exp_data["company"] = parts[0].strip()
                                        exp_data["employment_type"] = parts[1].strip()
                                    else:
                                        exp_data["company"] = company_text
                                elif company_text:
                                    exp_data["company"] = company_text
                        except:
                            pass
                        
                        # T√¨m th·ªùi gian
                        try:
                            # T√¨m span c√≥ class pvs-entity__caption-wrapper
                            time_span = item.find("span", {"class": lambda x: x and "pvs-entity__caption-wrapper" in str(x)})
                            if time_span:
                                time_text = time_span.get_text(strip=True)
                                if time_text:
                                    exp_data["duration"] = time_text
                        except:
                            pass
                        
                        # Ki·ªÉm tra xem ƒë√¢y c√≥ th·ª±c s·ª± l√† kinh nghi·ªám kh√¥ng
                        # Lo·∫°i b·ªè n·∫øu:
                        # 1. C√≥ ch·ª©a t·ª´ kh√≥a kh√¥ng ph·∫£i kinh nghi·ªám
                        # 2. L√† th√¥ng tin tr∆∞·ªùng h·ªçc
                        # 3. L√† th√¥ng tin ch·ª©ng ch·ªâ
                        
                        should_add = True
                        skip_keywords = ["university", "college", "school", "academy", "institute", 
                                        "certified", "certification", "license", "ch·ª©ng ch·ªâ", "b·∫±ng c·∫•p",
                                        "follower", "following", "theo d√µi", "ng∆∞·ªùi theo d√µi", "th√†nh vi√™n",
                                        "member", "connect", "k·∫øt n·ªëi"]
                        
                        # Ki·ªÉm tra position
                        if exp_data["position"]:
                            pos_lower = exp_data["position"].lower()
                            for keyword in skip_keywords:
                                if keyword in pos_lower:
                                    should_add = False
                                    break
                        
                        # Ki·ªÉm tra company
                        if exp_data["company"] and should_add:
                            company_lower = exp_data["company"].lower()
                            for keyword in skip_keywords:
                                if keyword in company_lower:
                                    should_add = False
                                    break
                        
                        # Ki·ªÉm tra xem c√≥ ph·∫£i l√† th√¥ng tin h·ªçc v·∫•n kh√¥ng
                        if exp_data["position"] and any(word in exp_data["position"].lower() for word in ["bachelor", "master", "phd", "degree", "student"]):
                            should_add = False
                        
                        # Ch·ªâ th√™m n·∫øu c√≥ √≠t nh·∫•t position ho·∫∑c company v√† kh√¥ng b·ªã skip
                        if should_add and (exp_data["position"] or exp_data["company"]):
                            # Lo·∫°i b·ªè c√°c m·ª•c tr√πng l·∫∑p ho·∫∑c kh√¥ng r√µ r√†ng
                            if exp_data["position"] and exp_data["company"]:
                                # Ki·ªÉm tra xem c√≥ ph·∫£i l√† th√¥ng tin h·ª£p l·ªá kh√¥ng
                                if len(exp_data["position"]) > 2 and len(exp_data["company"]) > 2:
                                    profile_data["experience"].append(exp_data)
        
        except Exception as e:
            print(f"    L·ªói khi l·∫•y kinh nghi·ªám: {e}")
        
        # 5. L·∫§Y H·ªåC V·∫§N (EDUCATION) - C√ÅCH TI·∫æP C·∫¨N M·ªöI
        print("    ƒêang l·∫•y th√¥ng tin h·ªçc v·∫•n...")
        try:
            # T√¨m t·∫•t c·∫£ c√°c section v√† l·ªçc ra section h·ªçc v·∫•n
            all_sections = soup.find_all("section")
            
            for section in all_sections:
                # Ki·ªÉm tra ti√™u ƒë·ªÅ c·ªßa section
                section_title = None
                h2_tag = section.find("h2")
                if h2_tag:
                    section_title = h2_tag.get_text(strip=True).lower()
                
                # Ki·ªÉm tra n·∫øu l√† section h·ªçc v·∫•n
                if section_title and ("education" in section_title or "h·ªçc v·∫•n" in section_title or "background" in section_title):
                    print(f"    T√¨m th·∫•y section h·ªçc v·∫•n: {section_title}")
                    
                    # T√¨m t·∫•t c·∫£ c√°c m·ª•c trong section n√†y
                    edu_items = []
                    
                    # C√°ch 1: T√¨m c√°c li items
                    edu_items = section.find_all("li", {"class": lambda x: x and any(cls in str(x) for cls in ["artdeco-list__item", "pvs-list__item"])})
                    
                    # C√°ch 2: T√¨m c√°c div c√≥ c·∫•u tr√∫c c·ª• th·ªÉ
                    if not edu_items:
                        edu_items = section.find_all("div", {"class": lambda x: x and "display-flex flex-column" in str(x)})
                    
                    # C√°ch 3: T√¨m c√°c a tag
                    if not edu_items:
                        edu_items = section.find_all("a", {"class": lambda x: x and "optional-action-target-wrapper" in str(x)})
                    
                    print(f"    T√¨m th·∫•y {len(edu_items)} m·ª•c h·ªçc v·∫•n")
                    
                    for item in edu_items:
                        # B·ªè qua c√°c item qu√° nh·ªè
                        item_text = item.get_text(strip=True)
                        if len(item_text) < 10:
                            continue
                        
                        edu_data = {
                            "school": None,
                            "degree": None,
                            "duration": None
                        }
                        
                        # T√¨m t√™n tr∆∞·ªùng
                        try:
                            # T√¨m trong th·∫ª span v·ªõi aria-hidden="true" trong div c√≥ class hoverable-link-text t-bold
                            school_div = item.find("div", {"class": lambda x: x and "hoverable-link-text" in str(x) and "t-bold" in str(x)})
                            if school_div:
                                school_span = school_div.find("span", {"aria-hidden": "true"})
                                if school_span:
                                    school_text = school_span.get_text(strip=True)
                                    if school_text:
                                        edu_data["school"] = school_text
                        except:
                            pass
                        
                        # T√¨m ng√†nh h·ªçc
                        try:
                            # T√¨m span c√≥ class t-14 t-normal
                            degree_span = item.find("span", {"class": lambda x: x and "t-14" in str(x) and "t-normal" in str(x)})
                            if degree_span:
                                degree_text = degree_span.get_text(strip=True)
                                if degree_text:
                                    edu_data["degree"] = degree_text
                        except:
                            pass
                        
                        # T√¨m th·ªùi gian
                        try:
                            # T√¨m span c√≥ class pvs-entity__caption-wrapper
                            time_span = item.find("span", {"class": lambda x: x and "pvs-entity__caption-wrapper" in str(x)})
                            if time_span:
                                time_text = time_span.get_text(strip=True)
                                if time_text:
                                    edu_data["duration"] = time_text
                        except:
                            pass
                        
                        # Ki·ªÉm tra xem ƒë√¢y c√≥ th·ª±c s·ª± l√† h·ªçc v·∫•n kh√¥ng
                        # Lo·∫°i b·ªè n·∫øu:
                        # 1. L√† th√¥ng tin c√¥ng ty
                        # 2. L√† th√¥ng tin kinh nghi·ªám
                        # 3. L√† th√¥ng tin ch·ª©ng ch·ªâ
                        
                        should_add = True
                        
                        # Ki·ªÉm tra school
                        if edu_data["school"]:
                            school_lower = edu_data["school"].lower()
                            # Lo·∫°i b·ªè n·∫øu l√† t√™n c√¥ng ty ho·∫∑c th√¥ng tin kh√¥ng ph·∫£i tr∆∞·ªùng h·ªçc
                            company_keywords = ["gmbh", "ltd", "inc", "corp", "company", "technologies", 
                                              "software", "solution", "group", "holding", "consulting"]
                            non_edu_keywords = ["follower", "following", "theo d√µi", "ng∆∞·ªùi theo d√µi", 
                                              "th√†nh vi√™n", "member", "connect", "k·∫øt n·ªëi", "certified"]
                            
                            for keyword in company_keywords + non_edu_keywords:
                                if keyword in school_lower:
                                    should_add = False
                                    break
                        
                        # Ki·ªÉm tra degree
                        if edu_data["degree"] and should_add:
                            degree_lower = edu_data["degree"].lower()
                            # Lo·∫°i b·ªè n·∫øu l√† lo·∫°i h√¨nh l√†m vi·ªác
                            work_keywords = ["full-time", "part-time", "internship", "contract", "temporary", 
                                           "freelance", "remote", "onsite"]
                            for keyword in work_keywords:
                                if keyword in degree_lower:
                                    should_add = False
                                    break
                        
                        # Ch·ªâ th√™m n·∫øu c√≥ school v√† kh√¥ng b·ªã skip
                        if should_add and edu_data["school"]:
                            # Lo·∫°i b·ªè c√°c m·ª•c tr√πng l·∫∑p ho·∫∑c kh√¥ng r√µ r√†ng
                            if len(edu_data["school"]) > 2:
                                # Ki·ªÉm tra xem school c√≥ ch·ª©a t·ª´ kh√≥a tr∆∞·ªùng h·ªçc kh√¥ng
                                edu_keywords = ["university", "college", "school", "academy", "institute", 
                                              "ƒë·∫°i h·ªçc", "cao ƒë·∫≥ng", "tr∆∞·ªùng", "h·ªçc vi·ªán"]
                                has_edu_keyword = any(keyword in edu_data["school"].lower() for keyword in edu_keywords)
                                
                                # N·∫øu kh√¥ng c√≥ t·ª´ kh√≥a tr∆∞·ªùng h·ªçc nh∆∞ng v·∫´n c√≥ th·ªÉ l√† t√™n tr∆∞·ªùng ng·∫Øn
                                if has_edu_keyword or len(edu_data["school"].split()) <= 5:
                                    profile_data["education"].append(edu_data)
        
        except Exception as e:
            print(f"    L·ªói khi l·∫•y h·ªçc v·∫•n: {e}")
        
        # 6. L·ªåC L·∫†I D·ªÆ LI·ªÜU ƒê·ªÇ LO·∫†I B·ªé C√ÅC M·ª§C KH√îNG PH√ô H·ª¢P
        # L·ªçc education: lo·∫°i b·ªè c√°c m·ª•c c√≥ ch·ª©a s·ªë l∆∞·ª£ng ng∆∞·ªùi theo d√µi, th√†nh vi√™n, v.v.
        filtered_education = []
        for edu in profile_data["education"]:
            school = edu.get("school", "")
            degree = edu.get("degree", "")
            
            # Lo·∫°i b·ªè n·∫øu ch·ª©a th√¥ng tin kh√¥ng ph√π h·ª£p
            skip_patterns = [
                "ng∆∞·ªùi theo d√µi", "th√†nh vi√™n", "follower", "member", "theo d√µi",
                "k·∫øt n·ªëi", "connect", "ƒëƒÉng", "newsletter", "wrap-up"
            ]
            
            should_skip = False
            for pattern in skip_patterns:
                if school and pattern in school.lower():
                    should_skip = True
                    break
                if degree and pattern in degree.lower():
                    should_skip = True
                    break
            
            # Lo·∫°i b·ªè n·∫øu l√† s·ªë l∆∞·ª£ng l·ªõn (c√≥ ch·ª©a d·∫•u ch·∫•m ho·∫∑c d·∫•u ph·∫©y trong s·ªë)
            if school and any(char.isdigit() for char in school):
                # Ki·ªÉm tra xem c√≥ ph·∫£i l√† s·ªë l∆∞·ª£ng kh√¥ng
                if "." in school or "," in school or any(word in school.lower() for word in ["k", "m", "tri·ªáu", "t·ª∑"]):
                    should_skip = True
            
            if not should_skip:
                filtered_education.append(edu)
        
        profile_data["education"] = filtered_education
        
        # L·ªçc experience: lo·∫°i b·ªè c√°c m·ª•c kh√¥ng ph·∫£i kinh nghi·ªám
        filtered_experience = []
        for exp in profile_data["experience"]:
            position = exp.get("position", "")
            company = exp.get("company", "")
            
            # Lo·∫°i b·ªè n·∫øu l√† th√¥ng tin h·ªçc v·∫•n
            edu_keywords = ["university", "college", "school", "academy", "institute", 
                          "bachelor", "master", "phd", "student", "candidate"]
            
            should_skip = False
            for keyword in edu_keywords:
                if position and keyword in position.lower():
                    should_skip = True
                    break
                if company and keyword in company.lower():
                    should_skip = True
                    break
            
            # Lo·∫°i b·ªè n·∫øu c√≥ ch·ª©a th·ªùi gian trong position ho·∫∑c company
            time_keywords = ["thg", "nƒÉm", "th√°ng", "hi·ªán t·∫°i", "present", "ƒë·∫øn", "t·ª´"]
            for keyword in time_keywords:
                if position and keyword in position.lower():
                    should_skip = True
                    break
                if company and keyword in company.lower():
                    should_skip = True
                    break
            
            if not should_skip:
                filtered_experience.append(exp)
        
        profile_data["experience"] = filtered_experience
        
        # C·∫¨P NH·∫¨T t·ªïng s·ªë kinh nghi·ªám
        profile_data["total_experience_count"] = len(profile_data["experience"])
        
        
        # Hi·ªÉn th·ªã th√¥ng tin ƒë√£ l·∫•y ƒë∆∞·ª£c
        print(f"    ƒê√£ l·∫•y ƒë∆∞·ª£c:")
        print(f"      - T√™n: {profile_data['name']}")
        print(f"      - Ch·ª©c v·ª•: {profile_data['job_title'][:50] if profile_data['job_title'] else 'N/A'}...")
        print(f"      - ƒê·ªãa ƒëi·ªÉm: {profile_data['location']}")
        print(f"      - S·ªë tr∆∞·ªùng h·ªçc: {len(profile_data['education'])}")
        print(f"      - S·ªë kinh nghi·ªám: {profile_data['total_experience_count']} c√¥ng ty")
        
        # Hi·ªÉn th·ªã chi ti·∫øt h·ªçc v·∫•n v√† kinh nghi·ªám
        if profile_data['education']:
            print(f"      - Chi ti·∫øt h·ªçc v·∫•n:")
            for i, edu in enumerate(profile_data['education'][:2], 1):
                print(f"        {i}. Tr∆∞·ªùng: {edu.get('school', 'N/A')}")
                print(f"           Ng√†nh: {edu.get('degree', 'N/A')}")
                print(f"           Th·ªùi gian: {edu.get('duration', 'N/A')}")
        
        if profile_data['experience']:
            print(f"      - Chi ti·∫øt kinh nghi·ªám ({profile_data['total_experience_count']} c√¥ng ty):")
            for i, exp in enumerate(profile_data['experience'][:2], 1):
                print(f"        {i}. V·ªã tr√≠: {exp.get('position', 'N/A')}")
                print(f"           C√¥ng ty: {exp.get('company', 'N/A')}")
                print(f"           Lo·∫°i h√¨nh: {exp.get('employment_type', 'N/A')}")
                print(f"           Th·ªùi gian: {exp.get('duration', 'N/A')}")

        # ===== G·ª¨I KAFKA =====
        if 'producer' in globals():
            producer.send(KAFKA_TOPIC, value=profile_data)
        print("    üì§ ƒê√£ g·ª≠i Kafka")

        time.sleep(random.uniform(4,6))  # delay m·∫°nh h∆°n

        return profile_data
    except Exception as e:
        print(f"‚ùå L·ªói {linkedin_URL}: {e}")
        return None

    finally:
        try:
            thread_driver.quit()
        except Exception:
            pass
# Task 4: Scrape the data c·ªßa t·ª´ng profile
print("\n=== B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu t·ª´ng profile ===")
profiles_data = []
total_profiles = len(URLs_all_page)

print("\nüöÄ B·∫Øt ƒë·∫ßu crawl ƒëa lu·ªìng (5 threads)...")

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = []

    for idx, url in enumerate(URLs_all_page, 1):
        if stop_flag:
            break

        futures.append(
            executor.submit(crawl_profile, url, idx, total_profiles)
        )

        time.sleep(random.uniform(2, 4))

    for future in as_completed(futures):
        try:
            result = future.result()
            if result:
                profiles_data.append(result)
                # C·∫≠p nh·∫≠t metadata
                url_clean = result['url'].split('?')[0]
                meta[url_clean] = {
                    'last_crawled': datetime.now().isoformat(),
                    'checksum': result.get('_checksum', '')
                }
                # Ghi metadata an to√†n v·ªõi lock
                with file_lock:
                    with open(META_PATH, 'w', encoding='utf-8') as f:
                        json.dump(meta, f, indent=4)
        except Exception as e:
            print(f"Thread error: {e}")
            continue

# EXPORT JSON FINAL
output_path = r"D:\Hoc_tap\linkedlin\Data\output.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(profiles_data, f, ensure_ascii=False, indent=4)

print(f"\n=== HO√ÄN TH√ÄNH ===")
print(f"ƒê√£ thu th·∫≠p d·ªØ li·ªáu c·ªßa {len(profiles_data)} profiles")
print(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_path}")

# Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu
if profiles_data:
    print("\n=== M·∫™U D·ªÆ LI·ªÜU ƒê√É THU TH·∫¨P ===")
    sample = profiles_data[0]
    print(f"T√™n: {sample.get('name', 'N/A')}")
    print(f"ƒê·ªãa ƒëi·ªÉm: {sample.get('location', 'N/A')}")
    print(f"Ch·ª©c v·ª•: {sample.get('job_title', 'N/A')}")
    print(f"T·ªïng s·ªë kinh nghi·ªám: {sample.get('total_experience_count', 0)} c√¥ng ty")
    
    if sample.get('experience'):
        print(f"\nKinh nghi·ªám ({len(sample['experience'])} c√¥ng ty):")
        for i, exp in enumerate(sample['experience'][:5], 1):
            print(f"  {i}. V·ªã tr√≠: {exp.get('position', 'N/A')}")
            print(f"     C√¥ng ty: {exp.get('company', 'N/A')}")
            print(f"     Lo·∫°i h√¨nh: {exp.get('employment_type', 'N/A')}")
            print(f"     Th·ªùi gian: {exp.get('duration', 'N/A')}")
    
    if sample.get('education'):
        print(f"\nH·ªçc v·∫•n ({len(sample['education'])} tr∆∞·ªùng):")
        for i, edu in enumerate(sample['education'][:3], 1):
            print(f"  {i}. Tr∆∞·ªùng: {edu.get('school', 'N/A')}")
            print(f"     Ng√†nh h·ªçc: {edu.get('degree', 'N/A')}")
            print(f"     Th·ªùi gian: {edu.get('duration', 'N/A')}")

# ƒê√≥ng tr√¨nh duy·ªát
print("\nƒêang ƒë√≥ng tr√¨nh duy·ªát...")
if 'producer' in globals():
    print("ƒêang ƒë√≥ng Kafka producer...")
    producer.flush(timeout=5)
    producer.close()
driver.quit()